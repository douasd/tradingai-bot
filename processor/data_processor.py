# tradingai-bot/processor/data_processor.py

import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from ta import add_all_ta_features
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import logging
import json
from utils.processor_error_handler import handle_errors, ProcessorDataError, MissingDataError, InvalidDataError

# Ortam DeÄŸiÅŸkenlerini YÃ¼kleme
load_dotenv()

# Ortam DeÄŸiÅŸkenleri
PROCESSED_DATA_PATH = os.getenv("PROCESSED_DATA_PATH", "data/processed")
HISTORICAL_DATA_PATH = os.getenv("HISTORICAL_DATA_PATH", "data/historical")
REALTIME_DATA_PATH = os.getenv("REALTIME_DATA_PATH", "data/realtime")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# Log YapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(f"logs/data_processor_{datetime.now().strftime('%Y%m%d')}.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("DataProcessor")

# ================================
# ðŸ“Š Veri Temizleme ve HazÄ±rlama
# ================================

class DataProcessor:
    def __init__(self):
        self.historical_data_path = Path(HISTORICAL_DATA_PATH)
        self.realtime_data_path = Path(REALTIME_DATA_PATH)
        self.processed_data_path = Path(PROCESSED_DATA_PATH)
        self.processed_data_path.mkdir(parents=True, exist_ok=True)

    @handle_errors
    def load_data(self, file_path: Path) -> pd.DataFrame:
        """Verileri okur ve temizlenmiÅŸ bir DataFrame dÃ¶ndÃ¼rÃ¼r"""
        if not file_path.exists():
            raise MissingDataError(f"{file_path} bulunamadÄ±.")
        
        logger.info(f"{file_path} yÃ¼kleniyor...")
        df = pd.read_parquet(file_path)
        df = self._clean_data(df)
        return df

    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Veri temizleme iÅŸlemleri"""
        if df.isnull().sum().sum() > 0:
            logger.warning("Eksik veriler tespit edildi, dolduruluyor...")
            df.fillna(method='ffill', inplace=True)
            df.fillna(method='bfill', inplace=True)
        
        if df.duplicated().any():
            logger.warning("Yinelenen veriler tespit edildi ve kaldÄ±rÄ±ldÄ±.")
            df.drop_duplicates(inplace=True)
        
        if not pd.api.types.is_datetime64_any_dtype(df['open_time']):
            df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')

        df.set_index('open_time', inplace=True)
        return df

    # ================================
    # ðŸ“ˆ Teknik GÃ¶stergeler Hesaplama
    # ================================

    @handle_errors
    def compute_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """TÃ¼m teknik gÃ¶stergeleri hesaplar"""
        logger.info("Teknik gÃ¶stergeler hesaplanÄ±yor...")
        df = add_all_ta_features(
            df, open="open", high="high", low="low", close="close", volume="volume", fillna=True
        )
        logger.info("Teknik gÃ¶stergeler hesaplandÄ±.")
        return df

    # ================================
    # ðŸ” Veri DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve Ã–lÃ§ekleme
    # ================================

    @handle_errors
    def normalize_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Veriyi normalize eder"""
        logger.info("Veri normalize ediliyor...")
        scaler = MinMaxScaler()
        scaled_values = scaler.fit_transform(df.select_dtypes(include=[np.number]))
        df_scaled = pd.DataFrame(scaled_values, index=df.index, columns=df.select_dtypes(include=[np.number]).columns)
        logger.info("Veri normalizasyonu tamamlandÄ±.")
        return df_scaled

    # ================================
    # ðŸ’¾ Veriyi Kaydetme
    # ================================

    @handle_errors
    def save_processed_data(self, df: pd.DataFrame, filename: str):
        """Ä°ÅŸlenmiÅŸ verileri kaydeder"""
        file_path = self.processed_data_path / f"{filename}.parquet"
        df.to_parquet(file_path, compression="zstd")
        logger.info(f"Ä°ÅŸlenmiÅŸ veri kaydedildi: {file_path}")

    # ================================
    # ðŸš€ Ä°ÅŸleme AkÄ±ÅŸÄ±
    # ================================

    def process_all_data(self):
        """TÃ¼m veri iÅŸleme akÄ±ÅŸÄ±"""
        logger.info("Veri iÅŸleme sÃ¼reci baÅŸlatÄ±ldÄ±...")

        # TÃ¼m tarihsel verileri iÅŸleme
        for file in self.historical_data_path.glob("**/*.parquet"):
            df = self.load_data(file)
            df = self.compute_technical_indicators(df)
            df = self.normalize_data(df)
            self.save_processed_data(df, file.stem)

        # TÃ¼m gerÃ§ek zamanlÄ± verileri iÅŸleme
        for file in self.realtime_data_path.glob("**/*.json.gz"):
            df = self.load_data(file)
            df = self.compute_technical_indicators(df)
            df = self.normalize_data(df)
            self.save_processed_data(df, file.stem)

        logger.info("Veri iÅŸleme tamamlandÄ±.")


# =======================================
# ðŸš€ Ana Ã‡alÄ±ÅŸtÄ±rma Fonksiyonu
# =======================================

if __name__ == "__main__":
    processor = DataProcessor()
    processor.process_all_data()
